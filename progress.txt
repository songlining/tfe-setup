# TFE on Kubernetes Setup - Progress Log
# ========================================
# This file tracks the progress of each iteration.
# Agents MUST append to this file after completing each story.
# Format: [ITERATION N] Story-X: Title - STATUS
# Never delete previous entries - this is an append-only log.

[ITERATION 1] Story-1: Create Kind Cluster - COMPLETE
-------------------------------------------
What was implemented:
- Created kind cluster named 'tfe' using a custom cluster configuration
- Cluster config file: manifests/kind/cluster-config.yaml
- Configured extra port mappings for nginx ingress (ports 80, 443, 30443)
- Added node label 'ingress-ready=true' for nginx ingress controller

Verification results:
- Cluster is running with Kubernetes v1.33.1
- Node tfe-control-plane is Ready
- Allocatable resources: 12 CPUs, ~8GB memory, 110 pods capacity
- All system pods (CoreDNS, etcd, kube-apiserver, etc.) are running

Learnings/Gotchas:
- Kind v0.29.0 uses kindest/node:v1.33.1 by default
- Node may take a few seconds to become Ready after cluster creation - use kubectl wait
- Port mappings must be configured at cluster creation time (cannot be changed later)
- The cluster config uses kind.x-k8s.io/v1alpha4 API version

Commands used:
- kind create cluster --config manifests/kind/cluster-config.yaml
- kubectl cluster-info --context kind-tfe
- kubectl get nodes --context kind-tfe
- kubectl describe node tfe-control-plane --context kind-tfe

[ITERATION 2] Story-2: Install dnsmasq DNS Server - COMPLETE
-------------------------------------------
What was implemented:
- Created 'dns' namespace for dnsmasq deployment
- Created ConfigMap (manifests/dns/configmap.yaml) with dnsmasq configuration:
  - Upstream DNS servers (Google DNS 8.8.8.8 and 8.8.4.4)
  - Local domain resolution for .local suffix
  - Query logging enabled for debugging
  - Additional hosts file for custom DNS entries
- Created Deployment (manifests/dns/deployment.yaml) using dockurr/dnsmasq:latest image
- Created ClusterIP Service (manifests/dns/service.yaml) exposing ports 53/UDP and 53/TCP
- Created Kustomization file (manifests/dns/kustomization.yaml) for easy deployment

Verification results:
- dnsmasq pod is running (1/1 Ready) in the 'dns' namespace
- Service has ClusterIP 10.96.83.66 (may vary on re-deployment)
- DNS resolution works for external domains (google.com, github.com)
- Service is accessible from other namespaces via dnsmasq.dns.svc.cluster.local
- Logs show proper query forwarding and caching behavior

Learnings/Gotchas:
- dnsmasq is NOT meant to replace CoreDNS for Kubernetes internal service discovery
- dnsmasq purpose: provide custom DNS resolution for .local domains (tfe.local, minio.local, etc.)
- CoreDNS handles internal K8s service names (*.svc.cluster.local)
- To query dnsmasq from any namespace, use: dnsmasq.dns.svc.cluster.local
- The hosts file in ConfigMap can be updated later to add service IP mappings
- dockurr/dnsmasq:latest image works well for simple dnsmasq deployment
- NET_ADMIN capability is required for dnsmasq to bind to port 53

Commands used:
- kubectl create namespace dns --context kind-tfe
- kubectl apply -k /Users/larry.song/work/hashicorp/tfe-setup/manifests/dns --context kind-tfe
- kubectl wait --for=condition=available deployment/dnsmasq -n dns --timeout=120s --context kind-tfe
- kubectl run dns-test --rm -it --restart=Never --image=busybox:1.36 --context kind-tfe -- nslookup google.com dnsmasq.dns.svc.cluster.local

[ITERATION 3] Story-3: Install S3-Compatible Storage (MinIO) - COMPLETE
-------------------------------------------
What was implemented:
- Created 's3' namespace for MinIO deployment
- Created Kubernetes manifests in manifests/s3/:
  - secret.yaml: MinIO root credentials (minioadmin/minioadmin123)
  - pvc.yaml: 10Gi PersistentVolumeClaim for MinIO data
  - deployment.yaml: MinIO deployment with health probes
  - service.yaml: ClusterIP service exposing ports 9000 (API) and 9001 (Console)
  - kustomization.yaml: Kustomize configuration for easy deployment
  - create-bucket-job.yaml: Job to create the 'tfe' bucket
  - test-s3-operations-job.yaml: Job to verify S3 operations
  - README.md: Documentation for MinIO configuration and credentials

Verification results:
- Namespace 's3' is created and active
- MinIO pod is running (1/1 Ready) with health checks passing
- S3 API endpoint accessible at minio.s3.svc.cluster.local:9000
- MinIO Console accessible at minio.s3.svc.cluster.local:9001
- TFE bucket created successfully
- S3 operations tested: upload, list, download, delete all working

Access Credentials (stored in secret minio-credentials):
- Access Key: minioadmin
- Secret Key: minioadmin123

TFE Configuration Reference:
- Endpoint: http://minio.s3.svc.cluster.local:9000
- Bucket: tfe
- Region: us-east-1

Learnings/Gotchas:
- minio/minio:latest image works well for simple deployments (no Helm chart needed)
- MinIO requires --console-address flag to enable web console on a separate port
- Health probes: /minio/health/ready and /minio/health/live endpoints work well
- When running mc (MinIO client) in a pod, use --command -- /bin/sh -c '...' syntax
- The PVC uses 'standard' storageClassName which works with kind's local-path-provisioner
- Jobs with ttlSecondsAfterFinished auto-cleanup completed pods (useful for one-time operations)
- S3 API returns 400 Bad Request for unauthenticated root requests (this is expected behavior)

Commands used:
- kubectl create namespace s3 --context kind-tfe --dry-run=client -o yaml | kubectl apply -f -
- kubectl apply -k /Users/larry.song/work/hashicorp/tfe-setup/manifests/s3 --context kind-tfe
- kubectl wait --for=condition=available deployment/minio -n s3 --timeout=120s --context kind-tfe
- kubectl apply -f manifests/s3/create-bucket-job.yaml --context kind-tfe
- kubectl logs job/create-tfe-bucket -n s3 --context kind-tfe

[ITERATION 4] Story-4: Install Redis - COMPLETE
-------------------------------------------
What was implemented:
- Created 'redis' namespace for Redis deployment
- Created Kubernetes manifests in manifests/redis/:
  - secret.yaml: Redis password (redispassword123) stored securely
  - pvc.yaml: 5Gi PersistentVolumeClaim for Redis data persistence
  - deployment.yaml: Redis 7-alpine deployment with AOF persistence enabled
  - service.yaml: ClusterIP service exposing port 6379
  - kustomization.yaml: Kustomize configuration for easy deployment
- Redis configured with password authentication (--requirepass)
- AOF persistence enabled (--appendonly yes) for data durability

Verification results:
- Namespace 'redis' is created and active
- Redis pod is running (1/1 Ready) with health checks passing
- Redis service accessible at redis.redis.svc.cluster.local:6379
- PVC is bound with 5Gi storage
- Redis logs show successful startup: "Ready to accept connections tcp"
- PING/PONG test successful from default namespace
- SET/GET/DEL operations verified working
- Cross-namespace connectivity confirmed (tested from s3 namespace)

Access Credentials (stored in secret redis-credentials):
- Password: redispassword123

TFE Configuration Reference:
- Host: redis.redis.svc.cluster.local
- Port: 6379
- Password: redispassword123 (from secret redis-credentials)

Learnings/Gotchas:
- redis:7-alpine image is lightweight and works well for Kubernetes deployments
- Password must be passed via --requirepass flag in command args
- Use $(ENV_VAR) syntax in args to reference environment variables from secrets
- AOF persistence (appendonly yes) provides better durability than RDB snapshots
- tcpSocket probes work better than exec probes for Redis health checks
- The 'standard' storageClassName works with kind's local-path-provisioner (same as MinIO)
- Redis password warning about -a flag is expected and can be ignored in test contexts

Commands used:
- kubectl create namespace redis --context kind-tfe --dry-run=client -o yaml | kubectl apply -f -
- kubectl apply -k /Users/larry.song/work/hashicorp/tfe-setup/manifests/redis --context kind-tfe
- kubectl wait --for=condition=available deployment/redis -n redis --timeout=120s --context kind-tfe
- kubectl run redis-test --rm -it --restart=Never --image=redis:7-alpine --context kind-tfe -- redis-cli -h redis.redis.svc.cluster.local -a redispassword123 PING

[ITERATION 5] Story-5: Install PostgreSQL - COMPLETE
-------------------------------------------
What was implemented:
- Created 'psql' namespace for PostgreSQL deployment
- Created Kubernetes manifests in manifests/psql/:
  - secret.yaml: PostgreSQL credentials (tfe/tfepassword123) with database name 'tfe'
  - pvc.yaml: 10Gi PersistentVolumeClaim for PostgreSQL data
  - deployment.yaml: PostgreSQL 15-alpine deployment with health probes (pg_isready)
  - service.yaml: ClusterIP service exposing port 5432
  - kustomization.yaml: Kustomize configuration for easy deployment
- Database 'tfe' is created automatically via POSTGRES_DB environment variable
- PGDATA set to subdirectory to avoid permission issues with PVC root

Verification results:
- Namespace 'psql' is created and active
- PostgreSQL pod is running (1/1 Ready) with health checks passing
- PostgreSQL service accessible at postgresql.psql.svc.cluster.local:5432
- PVC is bound with 10Gi storage
- PostgreSQL logs show: "database system is ready to accept connections"
- Database 'tfe' exists and is accessible
- CRUD operations verified (CREATE TABLE, INSERT, SELECT, DROP TABLE)
- Cross-namespace connectivity confirmed (tested from s3 namespace)

Access Credentials (stored in secret postgresql-credentials):
- Username: tfe
- Password: tfepassword123
- Database: tfe

TFE Configuration Reference:
- Host: postgresql.psql.svc.cluster.local
- Port: 5432
- Database: tfe
- User: tfe
- Password: tfepassword123 (from secret postgresql-credentials)

Learnings/Gotchas:
- postgres:15-alpine image is lightweight and works well for Kubernetes deployments
- POSTGRES_DB environment variable auto-creates the database during initialization
- PGDATA must point to a subdirectory (e.g., /var/lib/postgresql/data/pgdata) when using PVC
  - The PVC root directory may have lost+found or other files that cause issues
- pg_isready is the proper way to check PostgreSQL health in container probes
- Use PGPASSWORD environment variable when testing with psql client from pods
- Use -i flag (not -it) with kubectl run when running non-interactive commands
- The 'standard' storageClassName works with kind's local-path-provisioner (same as MinIO/Redis)

Commands used:
- kubectl create namespace psql --context kind-tfe --dry-run=client -o yaml | kubectl apply -f -
- kubectl apply -k /Users/larry.song/work/hashicorp/tfe-setup/manifests/psql --context kind-tfe
- kubectl wait --for=condition=available deployment/postgresql -n psql --timeout=120s --context kind-tfe
- kubectl run psql-test --rm -i --restart=Never --image=postgres:15-alpine --env="PGPASSWORD=tfepassword123" --context kind-tfe -- psql -h postgresql.psql.svc.cluster.local -U tfe -d tfe -c '\l'


[ITERATION 6] Story-6: Install HashiCorp Vault for TLS Certificates - COMPLETE
-------------------------------------------
What was implemented:
- Namespace 'vault' already created and active
- Vault deployed using Helm chart (hashicorp/vault) in standalone mode
- Vault configured with 2Gi PVC for data persistence using 'standard' storageClass
- Vault initialized and unsealed (shares=1, threshold=1 for dev environment)
- PKI secrets engine enabled at 'pki/' (Root CA) path
- Root CA generated with:
  - Common Name: "TFE Root CA"
  - Organization: "HashiCorp TFE Lab"
  - TTL: 87600h (10 years)
  - Key size: 4096-bit RSA
- Intermediate CA created at 'pki_int/' path:
  - Common Name: "TFE Intermediate CA"
  - Generated CSR, signed by Root CA, and imported back
  - TTL: 43800h (5 years)
  - Key size: 4096-bit RSA
- PKI role 'tfe-cert' configured for certificate issuance:
  - Allowed domains: tfe.local
  - Allow subdomains: true
  - Allow bare domains: true
  - Max TTL: 720h (30 days)
  - Default TTL: 24h
- Created configure-vault-pki.sh script for reproducible setup

Verification results:
- Vault pod running: vault-0 (1/1 Ready)
- Vault status: Initialized=true, Sealed=false
- Root CA certificate verified and accessible
- Intermediate CA certificate verified and accessible
- Certificate issuance tested: successfully issued cert for tfe.tfe.local
- All CRL and issuing certificate URLs configured properly

Access Credentials (stored in secret vault-credentials):
- Root Token: hvs.1MmdQ3PhmwE9SnX309vLwEj2
- Unseal Key: 3k6WXVbFLuGKNNIa45qmjsHHouaH6pCGnVZi+dr0tl0=

Vault Configuration Reference:
- Endpoint: http://vault.vault.svc.cluster.local:8200
- UI: http://vault-ui.vault.svc.cluster.local:8200
- Root CA Path: pki/
- Intermediate CA Path: pki_int/
- Certificate Role: pki_int/roles/tfe-cert

Certificate Issuance Commands:
# Issue certificate for TFE:
vault write pki_int/issue/tfe-cert common_name="tfe.tfe.local" ttl=24h

# Get Root CA certificate:
vault read pki/cert/ca

# Get Intermediate CA certificate:
vault read pki_int/cert/ca

Learnings/Gotchas:
- Vault Helm chart uses StatefulSet, which creates pods with ordinal suffix (vault-0, vault-1, etc.)
- For standalone mode (non-HA), only vault-0 pod is created
- The vault-keys secret was pre-created with unseal key and root token
- PKI intermediate CA setup requires:
  1. Generate intermediate CSR
  2. Sign the CSR with Root CA
  3. Import the signed certificate back into intermediate CA
- The 'region' parameter in vault write commands produces a warning but doesn't affect functionality
- When running vault CLI in pods, use 'sh -c' wrapper to run multiple commands and handle environment variables
- hashicorp/vault:1.21.2 image includes jq, but need 'apk add jq' for some operations
- Vault PKI URLs need to point to the Kubernetes service FQDN for in-cluster access
- Certificate chains are returned automatically when issuing from intermediate CA
- Testing certificate issuance is critical to verify the entire PKI chain works end-to-end

Commands used:
- kubectl get pods -n vault --context kind-tfe
- kubectl exec -n vault vault-0 --context kind-tfe -- vault status
- kubectl exec -n vault vault-0 --context kind-tfe -- sh -c 'VAULT_TOKEN=... vault secrets list'
- kubectl run vault-pki-config --rm -i --restart=Never --image=hashicorp/vault:1.21.2 -- sh -c '...'

[ITERATION 7] Story-7: Prepare TFE Helm Values - COMPLETE
-------------------------------------------
What was implemented:
- Created 'tfe' manifests directory with Helm configuration files
- Created values.yaml with all required TFE Helm chart configurations:
  - Image configuration: hashicorp/terraform-enterprise:v202507-1
  - Resource requests: 4Gi memory, 1000m CPU; limits: 8Gi memory, 2000m CPU
  - Security context: runAsNonRoot=true, runAsUser=1000, fsGroup=1012
  - LoadBalancer service configuration with port 443/30443
- S3 storage configuration pointing to MinIO:
  - Endpoint: http://minio.s3.svc.cluster.local:9000
  - Bucket: tfe
  - Region: us-east-1
  - Credentials from minio-credentials secret
- Redis configuration:
  - Host: redis.redis.svc.cluster.local:6379
  - Password from redis-credentials secret
  - Auth enabled, TLS disabled
- PostgreSQL configuration:
  - Host: postgresql.psql.svc.cluster.local:5432
  - Database: tfe, User: tfe
  - Password from postgresql-credentials secret
  - SSL mode: disable
- TLS certificate configuration:
  - Certificate secret: terraform-enterprise-certificates
  - Placeholder for Vault-issued certificates
  - CA cert directory and filename configured
- Environment variables configured:
  - TFE_HOSTNAME: tfe.tfe.local
  - TFE_CAPACITY_CONCENCY: 10
  - TFE_CAPACITY_CPU: 1000
  - TFE_CAPACITY_MEMORY: 4096
  - TFE_IACT_SUBNETS: 0.0.0.0/0
  - TFE_IACT_TIME_LIMIT: 1209600 (14 days)
  - TFE_TLS_CIPHERS: secure cipher suite
- TFE license embedded as base64-encoded secret
- Created setup-tls-from-vault.sh script to fetch certificates from Vault
- Created README.md with comprehensive documentation

Files created:
- manifests/tfe/values.yaml - Main Helm values configuration
- manifests/tfe/setup-tls-from-vault.sh - TLS certificate setup script
- manifests/tfe/README.md - Comprehensive documentation

Verification results:
- values.yaml contains all required Helm chart configurations
- S3 storage points to MinIO with correct endpoint and credentials
- Redis points to redis.redis.svc.cluster.local:6379 with authentication
- PostgreSQL points to postgresql.psql.svc.cluster.local:5432 with database 'tfe'
- TLS configuration references Vault PKI certificates
- All required TFE environment variables are configured
- TFE license is embedded (base64 encoded from /Users/larry.song/work/hashicorp/tfe-setup/tfe.license)
- setup-tls-from-vault.sh script ready to fetch certificates from Vault PKI

TFE Configuration Reference:
- Image: hashicorp/terraform-enterprise:v202507-1
- Hostname: tfe.tfe.local
- Database: postgresql.psql.svc.cluster.local:5432/tfe (user: tfe)
- Redis: redis.redis.svc.cluster.local:6379 (password: redispassword123)
- S3: http://minio.s3.svc.cluster.local:9000 (bucket: tfe)
- License: Embedded in values.yaml as TFE_LICENSE secret

Learnings/Gotchas:
- TFE Helm chart requires base64-encoded values for secrets in values.yaml
- The image tag should match the license version format (vYYYYMM-#)
- License file must be base64 encoded before embedding in values.yaml
- For TLS, use Vault PKI intermediate CA to issue certificates with 90-day TTL
- The terraform-enterprise-certificates secret must contain: cert.pem, key.pem, ca.pem
- TFE_ENCRYPTION_PASSWORD is required and should be set to a secure value
- TFE_CAPACITY_CONCENCY (note the typo in the variable name from HashiCorp)
- Image pull secret 'terraform-enterprise' must be created before Helm install
- For development, TFE_IACT_SUBNETS can be set to 0.0.0.0/0 (no restriction)
- The setup-tls-from-vault.sh script automates certificate fetching from Vault
- For kind clusters, LoadBalancer service type will use port-forwarding or MetalLB
- PostgreSQL sslmode=disable is acceptable for local kind cluster development
- TFE uses two sets of env vars: env.variables (ConfigMap) and env.secrets (Secrets)

Commands used:
- base64 < /path/to/tfe.license - Encode license file
- kubectl get secret minio-credentials -n s3 -o jsonpath='{.data}'
- kubectl get secret redis-credentials -n redis -o jsonpath='{.data}'
- kubectl get secret postgresql-credentials -n psql -o jsonpath='{.data}'

Next Steps (Story-8):
1. Create TFE namespace: kubectl create namespace tfe
2. Create image pull secret with license file
3. Run setup-tls-from-vault.sh to get TLS certificates
4. Update values.yaml with certificate data (or use secret)
5. Add HashiCorp Helm repo: helm repo add hashicorp https://helm.releases.hashicorp.com
6. Install TFE: helm install terraform-enterprise hashicorp/terraform-enterprise -n tfe -f values.yaml

[ITERATION 8] Story-8: Deploy Terraform Enterprise - BLOCKED
-------------------------------------------
What was attempted:
- Created 'tfe' namespace
- Created 'terraform-enterprise' docker-registry secret using license file as password
- Ran setup-tls-from-vault.sh to fetch TLS certificates from Vault
- Created 'terraform-enterprise-certificates' secret with cert.pem, key.pem, ca.pem
- Added HashiCorp Helm repository (hashicorp https://helm.releases.hashicorp.com)
- Attempted to install TFE Helm chart with values.yaml

BLOCKING ISSUE - Architecture Mismatch:
- TFE images are ONLY available for amd64 architecture
- HashiCorp does NOT provide arm64 images for Terraform Enterprise
- The kind cluster on Apple Silicon (M1/M2/M3) creates arm64 nodes by default
- Attempted solutions:
  1. Pulling amd64 kind node image - Still runs as arm64 on Apple Silicon
  2. Installing binfmt emulation - Causes control plane crashes, not viable
- Image pull error: "no match for platform in manifest: not found"

Verified Image Registry Data:
- Checked HashiCorp image registry for available tags
- TFE v202507-1 exists but ONLY supports linux/amd64 platform
- Manifest shows: {"architecture":"amd64","os":"linux"}
- No arm64 images available in any TFE version (including beta tags)

REQUIRED RESOLUTION:
To deploy TFE on Apple Silicon, you MUST use one of these alternatives:
1. Cloud-based Kubernetes cluster (EKS, GKE, AKS, etc.) with amd64 nodes
2. VM-based local cluster (minikube with --driver=vmware or --driver=virtualbox)
3. Colima with explicit amd64 architecture: colima start --arch x86_64 --kubernetes
4. Lima with amd64 configuration

Files created/modified:
- manifests/tfe/values.yaml - Complete Helm values configuration (ready for use on amd64 cluster)
- manifests/tfe/setup-tls-from-vault.sh - Fixed vault-keys secret key reference (root_token vs vault-root-token)
- manifests/kind/cluster-config.yaml - No changes needed (but kind won't work for TFE on Apple Silicon)
- AGENTS.md - Updated with critical architecture requirement documentation

Learnings/Gotchas:
- **CRITICAL**: TFE requires amd64 nodes - this is a hard requirement from HashiCorp
- The vault-keys secret uses 'root_token' key, not 'vault-root-token' (fixed in setup-tls-from-vault.sh)
- Docker-registry secret password must be the RAW license file content, not base64 encoded
- kind on Apple Silicon cannot run amd64 workloads reliably through emulation
- DO NOT try binfmt/qemu emulation with kind - causes control plane instability
- For home lab on Apple Silicon: Use Colima or Lima with amd64, or a cloud-based cluster

Acceptance Criteria Status:
- TFE namespace is created: PASS (namespace 'tfe' exists)
- TFE Helm chart is deployed successfully: BLOCKED (architecture mismatch)
- TFE pods are running and healthy: BLOCKED (cannot pull image on arm64)
- TFE application logs show successful startup: NOT APPLICABLE
- TFE is listening on configured ports: NOT APPLICABLE

Commands used for verification:
- curl -s -u terraform:$(cat tfe.license) https://images.releases.hashicorp.com/v2/hashicorp/terraform-enterprise/tags/list
- curl -s -u terraform:$(cat tfe.license) https://images.releases.hashicorp.com/v2/hashicorp/terraform-enterprise/manifests/v202507-1
- kubectl get nodes -o jsonpath='{.items[0].status.nodeInfo.architecture}'
- helm install terraform-enterprise hashicorp/terraform-enterprise -n tfe -f values.yaml
